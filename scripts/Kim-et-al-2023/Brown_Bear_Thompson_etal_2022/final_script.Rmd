---
title: "final_script"
author: "Peter R. Thompson"
date: '2022-09-29'
output: html_document
---

This script will conduct all the analyses necessary to replicate the analyses conducted by Thompson et al. (2021) for one individual bear in the population. This individual (ID: GM1046) was chosen because it had more data than most of the population, having been collared for four conseucitve years. Its movement track is depicted in Figure 3 of Thompson et al. (2022). The steps reuqired to complete this analysis, given the necessary input data, are detailed below. This includes loading some necessary R packages and defining a number of fairly complex functions. See the comments below for further detail.

```{r packages, message = FALSE, warning = FALSE}
library(TMB) # Template Model Builder (TMB) is the primary engine we use to fit the very complex models. It employs automatic differentiation techniques to much more accurately approximate a likelihood function, which greatly aids in the speed and precision of numerical optimization. When using TMB, the likelihood functions are written in C++; the necessary files are included in the repository here.
library(circular) # necessary for loading in the von Mises distribution, our angular distribution of choice for characterizing turning angles
```

I've written a few (fairly large) functions that help us run our analysis in a streamlined fashion. These functions are omitted from the "knitted" version of this file but can be viewed in the source code (.Rmd) version.

```{r functions, echo = FALSE}
read_SSF_raw_data = function(dir) {
  # This function reads all the raw data necessary to run our models. The only argument is a folder containing all the necessary files, listed below. 
  #
  # dir: a directory that should contain the following files (listed below)
  
  # list of observed animal step lengths
  r_cases = read.csv(paste0(dir, '/r_cases.csv'))[, -1]
  # list of observed animal turning angles
  phi_cases = read.csv(paste0(dir, '/phi_cases.csv'))[, -1]
  # integer - number of available (control) points simulated per used point
  n_simmed = read.csv(paste0(dir, '/n_simmed.csv'))[, -1]
  # resource data (all covariates - tihs is a matrix) for all the used points to be fit in the model
  res_cases = read.csv(paste0(dir, '/res_cases.csv'))[, -1]
  # resource data for corresponding available / control points
  res_controls = read.csv(paste0(dir, '/res_controls.csv'))[, -1]
  # resource data for all points for which we have data - used only for generating information on previous experience
  res_cases_all = read.csv(paste0(dir, '/res_cases_all.csv'))[, -1]
  # x coordinate (in meters) of all observed locations - used for information on previous experience / spatial location of used points
  x_cases_all = read.csv(paste0(dir, '/x_cases_all.csv'))[, -1]
  # y coordinate (in meters) of all observed locations - used for information on previous experience / spatial location of used points
  y_cases_all = read.csv(paste0(dir, '/y_cases_all.csv'))[, -1]
  # x coordinate (in meters) of all available points
  x_controls = read.csv(paste0(dir, '/x_controls.csv'))[, -1]
  # y coordinate (in meters) of all available points
  y_controls = read.csv(paste0(dir, '/y_controls.csv'))[, -1]
  # time indices for each available point - this is so we know what "used" point they correspond to
  strata_controls = read.csv(paste0(dir, '/strata_controls.csv'))[, -1]
  
  return(list(r_cases = r_cases, phi_cases = phi_cases, n_simmed = n_simmed,
              res_cases = res_cases, res_controls = res_controls, 
              res_cases_all = res_cases_all, x_cases_all = x_cases_all,
              y_cases_all = y_cases_all, x_controls = x_controls,
              y_controls = y_controls, strata_controls = strata_controls))
  
}

generateFunction = function(data, model = 'null', mu_stationary = 0.01, delta = NULL) {
  # This is an "internal" function that we use to generate likelihood functions from TMB for each model.
  #
  # data: a list of necessary data vectors / matrices (of the form of something returned by read_SSF_raw_data)
  # models - which models do we fit? Should be some combination of "null", "resource", "memory", or "combination"
  # mu_stationary - step length for quasi stationary state (parameter is fixed in estimation; rho_s in equations in Thompson et al., 2021)
  # delta - starting state probability for each state - if left as NULL we calculate it based on the proportion of steps with basically zero movement
  
  results = data.frame()
  
  # Load in our data from the given object
  r_cases = data$r_cases
  phi_cases = data$phi_cases
  n_simmed = data$n_simmed
  res_cases = data$res_cases
  res_controls = data$res_controls
  res_cases_all = data$res_cases_all
  x_cases_all = data$x_cases
  y_cases_all = data$y_cases
  x_controls = data$x_controls
  y_controls = data$y_controls
  strata_controls = data$strata_controls
  
  res_cases = as.matrix(res_cases)
  res_controls = as.matrix(res_controls)
  res_cases_all = as.matrix(res_cases_all)
  res_cases[is.na(res_cases)] = 0
  res_controls[is.na(res_controls)] = 0
  res_cases_all[is.na(res_cases_all)] = 0
  #with rasters these points may become NA on occasion, but shouldn't happen with the bear data
  
  #parameters
  mean_steplength = mean(r_cases)
  if (is.null(delta)) {
    #if we don't specify already
    prop_stationary = mean(r_cases < mu_stationary)
    delta = c(prop_stationary, 1-prop_stationary)
  }
  kappa_turningangle = mle.vonmises(phi_cases)$kappa
  beta_res_init = rep(0, ncol(res_cases))
  burnin = min(strata_controls) # hypothetically at what time did we start fitting the data
  
  #this code must be ran in a directory where these files exist already!
  if (model == 'null') {
    null_file = 'grizzly_null_20200528'
    # Compile the C++ likelihood function, if it has not been compiled already. Note that if it has been compiled already, this does nothing.
    compile(paste0(null_file, '.cpp'))
    # Get data ready for the TMB function (note that the names of our list match the names of the TMB variables exactly)
    data_null = list('r_cases' = r_cases, 'phi_cases' = phi_cases, 'delta' = delta, 'sl_stationary' = mu_stationary)
    # Load in parameters (note again that the names must be the same as the names in the TMB function; also, the values given to these parameters will be the initial conditions for optimization unless they are changed)
    par_null = list('steplength' = mean_steplength, 'angularconcentration' = kappa_turningangle, 'logit_lambda' = 0, 'logit_gamma' = 0)
    dyn.load(dynlib(null_file))
    # MakeADFun gives us all we need for optimization and we pass this into fit_SSF
    return(MakeADFun(data = data_null, parameters = par_null, DLL = null_file))
  }
  
  if (model == 'resource') {
    res_file = 'grizzly_resource_20200616'
    compile(paste0(res_file, '.cpp'))
    data_res = list('n_simmed' = n_simmed, 'r_cases' = r_cases, 'phi_cases' = phi_cases, 'res_cases' = res_cases, 'res_controls' = res_controls, 
                    'delta' = delta, 'sl_stationary' = mu_stationary)
    par_res = list('steplength' = mean_steplength, 'angularconcentration' = kappa_turningangle, 'beta_res' = beta_res_init, 
                   'logit_lambda' = 0, 'logit_gamma' = 0)
    dyn.load(dynlib(res_file))
    return(MakeADFun(data = data_res, parameters = par_res, DLL = res_file))
  }
  
  if (model == 'memory') {
    mem_file = 'grizzly_1pkmem_20200805'
    compile(paste0(mem_file, '.cpp'))
    data_mem = list('n_simmed' = n_simmed, 'r_cases' = r_cases, 'phi_cases' = phi_cases, 
                    'x_cases' = x_cases_all, 'y_cases' = y_cases_all, 'x_controls' = x_controls, 'y_controls' = y_controls, 
                    'strata_controls' = strata_controls, 'delta' = delta, 'sl_stationary' = mu_stationary)
    par_mem = list('steplength' = mean_steplength, 'angularconcentration' = kappa_turningangle, 'dist_coef' = 0, 'mean_tau' = burnin / 2,
                   'sd_tau' = burnin / 2, 'logit_lambda' = 0, 'logit_gamma' = 0, 'log_alpha' = 0)
    dyn.load(dynlib(mem_file))
    return(MakeADFun(data = data_mem, parameters = par_mem, DLL = mem_file))
  }
  
  if (model == 'combination' | model == 'resource-memory') {
    comb_file = 'grizzly_1pkcomb_20200717'
    compile(paste0(comb_file, '.cpp'))
    data_comb = list('n_simmed' = n_simmed, 'r_cases' = r_cases, 'phi_cases' = phi_cases, 'x_cases' = x_cases_all, 'y_cases' = y_cases_all, 
                     'x_controls' = x_controls, 'y_controls' = y_controls, 'strata_controls' = strata_controls, 'res_casesfit' = res_cases,
                     'res_casesall' = res_cases_all, 'res_controls' = res_controls, 'delta' = delta, 'sl_stationary' = mu_stationary)
    par_comb = list('steplength' = mean_steplength, 'angularconcentration' = kappa_turningangle, 'threshold' = 0, 'beta_res' = beta_res_init, 
                    'dist_coef' = 0, 'mean_tau' = burnin / 2, 'sd_tau' = burnin / 2, 'logit_lambda' = 0, 'logit_gamma' = 0, 'log_alpha' = 0)
    dyn.load(dynlib(comb_file))
    return(MakeADFun(data = data_comb, parameters = par_comb, DLL = comb_file))
  }
  
  stop('Invalid "model" command. Please select from the valid commands.')
  
}

fit_SSF = function(data, init_cond = 1, models = c('null', 'resource', 'memory', 'combination'), export_functions = FALSE, 
                   BIC = FALSE, lb_sdtau = 1, lb_mu = 5, mu_stationary = 0.01, delta = NULL, ...) {
  # This is the function we use to fit all the models (or any models we desire - see the 'models' argument) to the data. It is very customizable so see the arguments below for more information. The output is a data.frame with all model fitting results, including parameter estimates, confidence intervals, and AIC or BIC.
  #
  # data: a list of necessary data vectors / matrices (of the form of something returned by read_SSF_raw_data)
  # init_cond - for memory models, the number of starting conditions for mu. Typically this makes it less likely for us to get "stuck" in a local optimum
  # models - which models do we fit? Should be some combination of "null", "resource", "memory", or "combination"
  # export_functions - TRUE if we would like to access raw likelihood functions after model fitting (uses up a lot of memory); FALSE otherwise
  # BIC - TRUE if we would like our models to be compared using BIC, FALSE if we would like our models to be compared using AIC
  # lb_sdtau - lower bound for parameter sigma
  # lb_mu - lower bound for parameter mu
  # rm_s - do we remove stationary points from memory calculation?
  # mu_stationary - step length for quasi stationary state (parameter is fixed in estimation; rho_s in equations in Thompson et al., 2021)
  # delta - starting state probability for each state - if left as NULL we calculate it based on the proportion of steps with basically zero movement
  # ... - additional arguments to nlminb, the optimizing function (e.g., "control")
  
  results = data.frame()
  
  r_cases = data$r_cases
  strata_controls = data$strata_controls
  burnin = min(strata_controls) # hypothetically at what time did we start fitting the data
  
  #this code must be ran in a directory where these files exist already!
  if ('null' %in% models) {
    L_null = generateFunction(data = data, model = 'null', mu_stationary = mu_stationary, delta = delta)
    if (export_functions) L.null <<- L.null
    mle_null = nlminb(L_null$par, L_null$fn, L_null$gr, lower = c(0, 0, -10, -10), upper = c(Inf, Inf, Inf, Inf), ...)
    IC_null = ifelse(BIC, 2*mle_null$objective + log(length(r_cases))*length(unlist(L_null$par)), 
                     2*mle_null$objective + 2*length(unlist(L_null$par)))
    results_null = data.frame(model = 'null', IC = IC_null, estimate = mle_null$par, se = sdreport(L_null)$sd, 
                              msg = mle_null$message, const_l = c(0, 0, -10, -10), const_u = c(Inf, Inf, Inf, Inf))
    if (!export_functions) rm(L_null) # removing these big TMB objects saves a lot of memory so unless we want to keep them, remove it
    results = rbind(results, results_null)
  }
  
  if ('resource' %in% models) {
    L_res = generateFunction(data = data, model = 'resource', mu_stationary = mu_stationary, delta = delta)
    n_res = length(L_res$par)-4 # number of resource parameters (subtract rho, kappa, lambda, gamma)
    if (export_functions) L_res <<- L_res
    if (n_res == 6) {
      #we're working with the real bear data, and must implement special bounds to avoid optimization problems (large values for covariates)
      lower_res = c(0, 0, -Inf, -10, -Inf, -Inf, -10, -10, -10, -10)
      upper_res = c(Inf, Inf, Inf, 10, Inf, Inf, 10, 10, 10, 10)
    } else {
      lower_res = c(0,0,rep(-Inf, n_res), rep(-10, 2))
      upper_res = c(rep(Inf, n_res+2), rep(10, 2))
    }
    mle_res = nlminb(L_res$par, L_res$fn, L_res$gr, lower = lower_res, upper = upper_res, ...)
    IC_res = ifelse(BIC, 2*mle_res$objective + log(length(r_cases))*length(unlist(L_res$par)), 
                    2*mle_res$objective + 2*length(unlist(L_res$par)))
    results_res = data.frame(model = 'resource', IC = IC_res, estimate = mle_res$par, se = sdreport(L_res)$sd, 
                             msg = mle_res$message, const_l = lower_res, const_u = upper_res)
    if (!export_functions) rm(L_res)
    results = rbind(results, results_res)
  }
  
  if ('memory' %in% models | 'combination' %in% models) {
    # Get all initial conditions for mu, from the upper bound (burnin; the total amount of time we have pre-fitting) to the lower bound (lb_mu)
    init_cond_x = seq(burnin, lb_mu, length.out = init_cond)
  }
  
  if ('memory' %in% models) {
    L_mem = generateFunction(data = data, model = 'memory', mu_stationary = mu_stationary, delta = delta)
    if (export_functions) L_mem <<- L_mem
    mean_steplength = L_mem$par[1]
    upper_alpha = log(1 / mean_steplength)
    lower_mem = c(0, 0, 0, lb_mu, lb_sdtau, -10, -10, -Inf)
    upper_mem = c(Inf, Inf, 1000, burnin, Inf, Inf, Inf, upper_alpha)
    mle_mem = lapply(X = 1:(init_cond), FUN = function(x) {
      nlminb(c(L_mem$par[1:2], 50, init_cond_x[x], lb_sdtau * 2, 0, 0, upper_alpha), 
             L_mem$fn, L_mem$gr, lower = lower_mem, upper = upper_mem, ...)
    })
    logLiks_mem = unlist(lapply(X = mle_mem, FUN = function(m) {m$objective}))
    #find the intial condition that produced the best results and go with those parameters
    #do [1] in case there's a tie, don't want a list of 2 here
    mle_mem = mle_mem[[which(logLiks_mem == min(logLiks_mem[logLiks_mem > length(r_cases)]))[1]]]
    IC_mem = ifelse(BIC, 2*mle_mem$objective + log(length(r_cases))*length(unlist(L_mem$par)), 
                    2*mle_mem$objective + 2*length(unlist(L_mem$par)))
    results_mem = data.frame(model = 'memory-only', IC = IC_mem, estimate = mle_mem$par, se = sdreport(L_mem)$sd, 
                             msg = mle_mem$message, const_l = lower_mem, const_u = upper_mem)
    if (!export_functions) rm(L_mem)
    results = rbind(results, results_mem)
  }
  
  # Note here that "combination" is synonymous with "resource-memory"
  if ('combination' %in% models) {
    L_comb = generateFunction(data = data, model = 'combination', mu_stationary = mu_stationary, delta = delta)
    n_res = length(L_comb$par) - 9 # in the event that we don't run the resource-only model we re-define this
    if (export_functions) L_comb <<- L_comb
    mean_steplength = L_comb$par[1]
    upper_alpha = log(1/mean_steplength)
    
    if (n_res == 6) {
      #we're working with the real bear data, and must implement special bounds to avoid optimization problems (large values for covariates)
      lower_comb = c(0, 0, -Inf, -Inf, -10, -Inf, -Inf, -10, -10, 0, lb_mu, lb_sdtau, -10, -10, -Inf)
      upper_comb = c(Inf, Inf, Inf, Inf, 10, Inf, Inf, 10, 10, 1000, burnin, burnin, Inf, Inf, upper_alpha)
    } else {
      lower_comb = c(0,0,rep(-Inf, n_res+1), b_mem_low, lb_mu, lb_sdtau, -10, -10, -Inf)
      upper_comb = c(rep(Inf, n_res+3), 1000, burnin, burnin, Inf, Inf, upper_alpha)
    }
    
    mle_comb = lapply(X = 1:(init_cond), FUN = function(x) {
      start_comb1pk = c(L_comb$par[1:(3+n_res)], 50, init_cond_x[x], lb_sdtau * 2, 0, 0, min(0, upper_alpha))
      nlminb(start_comb1pk, L_comb$fn, L_comb$gr, lower = lower_comb, upper = upper_comb, ...)
    })
    logLiks_comb = unlist(lapply(X = mle_comb, FUN = function(m) {m$objective}))
    #do [1] in case there's a tie, don't want a list of 2 here
    mle_comb = mle_comb[[which(logLiks_comb == min(logLiks_comb[logLiks_comb > length(r_cases)]))[1]]]
    IC_comb = ifelse(BIC, 2*mle_comb$objective + log(length(r_cases))*length(L_comb$par), 
                     2*mle_comb$objective + 2*length(L_comb$par))
    results_comb = data.frame(model = 'resource-memory', IC = IC_comb, estimate = mle_comb$par, se = sdreport(L_comb)$sd, 
                              msg = mle_comb$message, const_l = lower_comb, const_u = upper_comb)     
    if (!export_functions) rm(L_comb)
    results = rbind(results, results_comb)
  }
  
  cbind(results, CL = results$estimate - 2*results$se, CU = results$estimate + 2*results$se)
  
}
```

Now that we have read in all our functions, we can conduct our analysis, displayed below. Before running this line you'll need to set your working directory to a folder that contains the folder "GM1046_input" as well as all the necessary C++ files.

```{r analysis, results = FALSE, warning = FALSE}
# First we read in our data and get it in the proper format for analysis.
data_raw = read_SSF_raw_data("GF1143_input")

# In this function, we fit all four models at once to our input data. See the above code block for further detail on what this function does (in short, it uses TMB to fit the SSF models to the data).
fit = fit_SSF(data = data_raw, init_cond = 3, lb_sdtau = 18, BIC = TRUE,  mu_stationary = 30, control = list(eval.max = 1000, iter.max = 1000),
              models = c('null', 'resource', 'memory', 'combination'))

# This allows us to store our results as a CSV file for further analysis
write.csv(fit, "model_fits_1143.csv")
```

Now we can analyze our results. What do we see? Let's first use BIC to identify which model produced the most parsimonious explanation of the patterns observed in this bear's movements. 

```{r bic}
fit[c(1, 5, 16, 30), c("model", "IC")]
```

We can see that here, the resource-memory model produced the lowest BIC. Let's look more at the parameter estimates for this model.

```{r estimates}
fit[fit$model == "resource-memory", c("estimate", "se", "CL", "CU")]
```

